{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b490b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from kernels.nn import ImplicitDenseNetKernel\n",
    "from benchmarks.cmgp_modified import CMGP\n",
    "from model.ick import ICK\n",
    "from model.cmick import CMICK\n",
    "from utils.train import CMICKEnsembleTrainer\n",
    "from utils.helpers import *\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(2020)\n",
    "torch.manual_seed(2020)\n",
    "torch.cuda.manual_seed(2020)\n",
    "torch.cuda.manual_seed_all(2020)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65853c21",
   "metadata": {},
   "source": [
    "# 1. Load IHDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2870e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(rep, batch_size=32):\n",
    "    # Load data repetition function\n",
    "    def load_data_rep(data_train, data_test, i):\n",
    "        # Training data\n",
    "        X_train = data_train['x'][:, :, i]\n",
    "        T_train = data_train['t'][:, i:i + 1]\n",
    "        Y_train = data_train['yf'][:, i:i + 1]\n",
    "        mu0_train = data_train['mu0'][:, i:i + 1]  # mu0 is the mean of control outcome\n",
    "        mu1_train = data_train['mu1'][:, i:i + 1]  # mu1 is the mean of treatment outcome\n",
    "\n",
    "        # Test data\n",
    "        X_test = data_test['x'][:, :, i]\n",
    "        T_test = data_test['t'][:, i:i + 1]\n",
    "        Y0_test = data_test['yf'][:, i:i + 1] * (1.0 - data_test['t'][:, i:i + 1])  # Y0 is the outcome for control\n",
    "        Y0_test += data_test['ycf'][:, i:i + 1] * data_test['t'][:, i:i + 1]\n",
    "        Y1_test = data_test['ycf'][:, i:i + 1] * (1.0 - data_test['t'][:, i:i + 1])  # Y1 is the outcome for treatment\n",
    "        Y1_test += data_test['yf'][:, i:i + 1] * data_test['t'][:, i:i + 1]\n",
    "        mu0_test = data_test['mu0'][:, i:i + 1]\n",
    "        mu1_test = data_test['mu1'][:, i:i + 1]\n",
    "\n",
    "        # Log-likelihood\n",
    "        ll_test = np.mean(np.log(norm.cdf(Y0_test - mu0_test + 0.5) - norm.cdf(Y0_test - mu0_test - 0.5))) + \\\n",
    "        np.mean(np.log(norm.cdf(Y1_test - mu1_test + 0.5) - norm.cdf(Y1_test - mu1_test - 0.5)))\n",
    "\n",
    "        # Return data\n",
    "        return X_train, T_train, Y_train, mu0_train, mu1_train, X_test, \\\n",
    "        T_test, Y0_test, Y1_test, mu0_test, mu1_test, ll_test\n",
    "\n",
    "    data_train = np.load('../../data/IHDP/ihdp_npci_1-100.train.npz', allow_pickle=True)\n",
    "    data_test = np.load('../../data/IHDP/ihdp_npci_1-100.test.npz', allow_pickle=True)\n",
    "\n",
    "    X_train, T_train, Y_train, mu0_train, mu1_train, X_test, T_test, \\\n",
    "    Y0_test, Y1_test, mu0_test, mu1_test, ll_test = load_data_rep(\n",
    "        data_train=data_train,\n",
    "        data_test=data_test,\n",
    "        i=rep\n",
    "    )\n",
    "    mu_test = mu1_test - mu0_test\n",
    "    data = {'X_train': X_train, 'T_train': T_train, 'Y_train': Y_train, 'X_test': X_test,\n",
    "            'mu0_test': mu0_test, 'mu1_test': mu1_test ,'mu_test': mu_test}\n",
    "\n",
    "    # Define dataset and dataloaders\n",
    "    data_train = [X_train, T_train]\n",
    "    data_test = [X_test, T_test]\n",
    "    Y_test = np.concatenate((Y0_test, Y1_test), axis=1)[range(len(Y0_test)),T_test.astype(np.int32).reshape(-1)].reshape(-1,1)\n",
    "    data_generators = create_generators_from_data(data_train, Y_train, data_test, Y_test, train_batch_size=batch_size)\n",
    "    return data_generators, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba6915",
   "metadata": {},
   "source": [
    "# 2. Define CMNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074462a",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "&f_1^1, f_2^1, f_3^1 \\sim K_1 \\\\\n",
    "&f_1^2, f_2^2, f_3^2 \\sim K_2 \\\\\n",
    "&Y_0(x) = \\alpha_1^1 f_1^1(x) + \\alpha_3^1 f_3^1(x) + 0 + \\alpha_1^2 f_1^2(x) + \\alpha_3^2 f_3^2(x) + 0 \\\\\n",
    "&Y_1(x) = 0 + \\alpha_2^1 f_2^1(x) + \\alpha_3^1 f_3^1(x) + 0 + \\alpha_2^2 f_2^2(x) + \\alpha_3^2 f_3^2(x)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549f4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cmnn_ensemble(input_dim, load_weights=False):\n",
    "    alpha11, alpha12, alpha13 = 1.0, 1.0, 1.0\n",
    "    alpha21, alpha22, alpha23 = 1.0, 1.0, 1.0\n",
    "    num_estimators = 10\n",
    "\n",
    "    ensemble, ensemble_weights = [], {}\n",
    "    for i in range(num_estimators):\n",
    "        f11 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 2048,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 2048, \n",
    "                    'dropout_ratio': 0.1, \n",
    "                    'activation': 'softplus'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        f12 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 2048,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 2048, \n",
    "                    'dropout_ratio': 0.1, \n",
    "                    'activation': 'softplus'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        f13 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 2048,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 2048, \n",
    "                    'dropout_ratio': 0.1, \n",
    "                    'activation': 'softplus'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        f21 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 2048,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 2048, \n",
    "                    'dropout_ratio': 0.1, \n",
    "                    'activation': 'softplus'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        f22 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 2048,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 2048, \n",
    "                    'dropout_ratio': 0.1, \n",
    "                    'activation': 'softplus'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        f23 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 2048,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 2048, \n",
    "                    'dropout_ratio': 0.1, \n",
    "                    'activation': 'softplus'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        if load_weights:\n",
    "            for f in ['f11', 'f12', 'f13', 'f21', 'f22', 'f23']:\n",
    "                eval(f).kernels[0].load_state_dict(torch.load('./checkpoints/cmick_ihdp.pt')['model_'+str(i+1)][f])\n",
    "        else:\n",
    "            model_weights = {\n",
    "                'f11': f11.kernels[0].state_dict(), 'f12': f12.kernels[0].state_dict(), 'f13': f13.kernels[0].state_dict(), \n",
    "                'f21': f21.kernels[0].state_dict(), 'f22': f22.kernels[0].state_dict(), 'f23': f23.kernels[0].state_dict()\n",
    "            }\n",
    "            ensemble_weights['model_'+str(i+1)] = model_weights\n",
    "#         f11.kernels[0].reset_parameters_normal(w_std=np.sqrt(0.05),b_std=np.sqrt(0.05))\n",
    "#         f12.kernels[0].reset_parameters_normal(w_std=np.sqrt(0.05),b_std=np.sqrt(0.05))\n",
    "#         f13.kernels[0].reset_parameters_normal(w_std=np.sqrt(0.05),b_std=np.sqrt(0.05))\n",
    "#         f21.kernels[0].reset_parameters_normal(w_std=np.sqrt(0.10),b_std=np.sqrt(0.10))\n",
    "#         f22.kernels[0].reset_parameters_normal(w_std=np.sqrt(0.10),b_std=np.sqrt(0.10))\n",
    "#         f23.kernels[0].reset_parameters_normal(w_std=np.sqrt(0.10),b_std=np.sqrt(0.10))\n",
    "        baselearner = CMICK(\n",
    "            control_components=[f11,f21], treatment_components=[f12,f22], shared_components=[f13,f23],\n",
    "            control_coeffs=[alpha11,alpha21], treatment_coeffs=[alpha12,alpha22], shared_coeffs=[alpha13,alpha23], \n",
    "            coeff_trainable=True\n",
    "        )\n",
    "        ensemble.append(baselearner)\n",
    "    if not load_weights:\n",
    "        if not os.path.exists('./checkpoints'):\n",
    "            os.makedirs('./checkpoints')\n",
    "        torch.save(ensemble_weights, './checkpoints/cmick_ihdp.pt')\n",
    "        \n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8c68d",
   "metadata": {},
   "source": [
    "# 3. Training and evaluation of CMNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d0d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_cmnn(ensemble, data_generators, mu_test, lr, treatment_index=1):\n",
    "    # The index of \"T_train\" in \"data_train\" is 1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optim = 'adam'\n",
    "    optim_params = {\n",
    "        'lr': lr, \n",
    "        # 'momentum': 0.99,\n",
    "        'weight_decay': 1e-3\n",
    "    }\n",
    "    epochs, patience = 1000, 30\n",
    "    trainer = CMICKEnsembleTrainer(\n",
    "        model=ensemble,\n",
    "        data_generators=data_generators,\n",
    "        optim=optim,\n",
    "        optim_params=optim_params, \n",
    "        model_save_dir=None,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        patience=patience, \n",
    "        treatment_index=treatment_index\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "    mean_test_pred, std_test_pred, y_test_true = trainer.predict()\n",
    "    mu_test_pred = mean_test_pred[:,1] - mean_test_pred[:,0]\n",
    "\n",
    "    # PEHE\n",
    "    pehe_test = np.sqrt(np.mean((mu_test_pred - mu_test) ** 2))\n",
    "    print('PEHE (CMNN):             %.4f' % (pehe_test))\n",
    "    \n",
    "    return pehe_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed4672",
   "metadata": {},
   "source": [
    "# 4. Benchmark 1: original CMGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e0ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_original_cmgp(data):\n",
    "    X_train, T_train, Y_train = data['X_train'], data['T_train'], data['Y_train']\n",
    "    X_test, mu_test = data['X_test'], data['mu_test']\n",
    "    cmgp_model = CMGP(X_train, T_train, Y_train)\n",
    "    \n",
    "    mu0_test_pred, mu1_test_pred = cmgp_model.predict(X_test, return_var=False)\n",
    "    mu_test_pred = mu1_test_pred - mu0_test_pred\n",
    "    pehe_test = np.sqrt(np.mean((mu_test_pred - mu_test) ** 2))\n",
    "    print('PEHE (CMGP):             %.4f' % (pehe_test))\n",
    "    return pehe_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc6a8d",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    n_reps = 5\n",
    "    lr = 1e-3\n",
    "    batch_size = 32\n",
    "    sqrt_pehe_cmnn_arr = np.zeros(n_reps)\n",
    "    sqrt_pehe_cmgp_arr = np.zeros(n_reps)\n",
    "    res = {'sqrt_pehe_mean': {}, 'sqrt_pehe_std': {}}\n",
    "    for i in range(n_reps):\n",
    "        data_generators, data = load_and_preprocess_data(rep=i, batch_size=batch_size)\n",
    "        input_dim = data['X_train'].shape[1]\n",
    "        ensemble = build_cmnn_ensemble(input_dim, load_weights=False)\n",
    "        sqrt_pehe_cmnn = fit_and_evaluate_cmnn(\n",
    "            ensemble, data_generators, data['mu_test'], lr=lr)\n",
    "        data_generators, data = load_and_preprocess_data(rep=i)\n",
    "        sqrt_pehe_cmgp = fit_and_evaluate_original_cmgp(data)\n",
    "        sqrt_pehe_cmnn_arr[i] = sqrt_pehe_cmnn\n",
    "        sqrt_pehe_cmgp_arr[i] = sqrt_pehe_cmgp\n",
    "    print('PEHE (CMNN):             %.4f +/- %.4f' % (np.mean(sqrt_pehe_cmnn_arr), np.std(sqrt_pehe_cmnn_arr)))\n",
    "    print('PEHE (CMGP):             %.4f +/- %.4f' % (np.mean(sqrt_pehe_cmgp_arr), np.std(sqrt_pehe_cmgp_arr)))\n",
    "    res['sqrt_pehe_mean']['cmnn'] = np.mean(sqrt_pehe_cmnn_arr)\n",
    "    res['sqrt_pehe_mean']['cmgp'] = np.mean(sqrt_pehe_cmgp_arr)\n",
    "    res['sqrt_pehe_std']['cmnn'] = np.std(sqrt_pehe_cmnn_arr)\n",
    "    res['sqrt_pehe_std']['cmgp'] = np.std(sqrt_pehe_cmgp_arr)\n",
    "    try:\n",
    "        os.makedirs('./results')\n",
    "    except FileExistsError:\n",
    "        print('Directory already exists.')\n",
    "    with open('./results/ihdp_results.pkl', 'wb') as fp:\n",
    "        pkl.dump(res, fp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
