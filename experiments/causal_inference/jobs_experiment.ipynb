{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f475b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import sys, random\n",
    "sys.path.insert(0, '../../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from kernels.nn import ImplicitDenseNetKernel\n",
    "from model.ick import ICK\n",
    "from model.cmick import CMICK\n",
    "from benchmarks.cmgp_modified import CMGP\n",
    "from benchmarks.cevae_modified import *\n",
    "from benchmarks.x_learner import X_Learner_RF, X_Learner_BART\n",
    "from ganite import Ganite\n",
    "from utils.train import CMICKEnsembleTrainer\n",
    "from utils.helpers import *\n",
    "from utils.metrics import policy_risk_binary\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "random.seed(2020)\n",
    "np.random.seed(2020)\n",
    "torch.manual_seed(2020)\n",
    "torch.cuda.manual_seed(2020)\n",
    "torch.cuda.manual_seed_all(2020)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ae8bf",
   "metadata": {},
   "source": [
    "# 1. Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6e90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(train_ratio, test_ratio, random_state):\n",
    "    randomized_data_dir = '../../data/JOBS/randomized.csv'\n",
    "    randomized_df = pd.read_csv(randomized_data_dir)\n",
    "    nonrandomized_data_dir = '../../data/JOBS/nonrandomized.csv'\n",
    "    nonrandomized_df = pd.read_csv(nonrandomized_data_dir)\n",
    "    df = pd.concat([randomized_df, nonrandomized_df], ignore_index=True)\n",
    "    cols_to_normalize = ['Age', 'Education', 'RE75']\n",
    "    for c in df.columns:\n",
    "        if c in cols_to_normalize:\n",
    "            scaler = StandardScaler()\n",
    "            df[c] = scaler.fit_transform(df[c].to_numpy().reshape(-1,1)).reshape(-1)\n",
    "    df['RE78'] = df['RE78'].apply(lambda x: 1. if x > 0. else 0.)   # Trasform to binary classification task\n",
    "    \n",
    "    N = len(df)\n",
    "    # Test set only from randomized samples\n",
    "    test_df = df[:len(randomized_df)].sample(n=int(test_ratio*N), random_state=random_state)  \n",
    "    df = df.drop(test_df.index)\n",
    "    train_df = df.sample(n=int(train_ratio*N), random_state=random_state)\n",
    "    val_df = df.drop(train_df.index)\n",
    "    X_train, X_val, X_test = train_df.to_numpy()[:,1:-1], val_df.to_numpy()[:,1:-1], test_df.to_numpy()[:,1:-1]\n",
    "    T_train, T_val, T_test = train_df.to_numpy()[:,:1], val_df.to_numpy()[:,:1], test_df.to_numpy()[:,:1]\n",
    "    Y_train, Y_val, Y_test = train_df.to_numpy()[:,-1:], val_df.to_numpy()[:,-1:], test_df.to_numpy()[:,-1:]\n",
    "    \n",
    "    data = {'X_train': X_train, 'T_train': T_train, 'Y_train': Y_train, 'X_val': X_val, 'T_val': T_val,\n",
    "            'Y_val': Y_val, 'X_test': X_test, 'T_test': T_test, 'Y_test': Y_test}\n",
    "    data_train, data_val, data_test = [X_train, T_train], [X_val, T_val], [X_test, T_test]\n",
    "    data_generators = create_generators_from_data(\n",
    "        x_train=data_train, y_train=Y_train, \n",
    "        x_val=data_val, y_val=Y_val,\n",
    "        x_test=data_test, y_test=Y_test\n",
    "    )\n",
    "    return data_generators, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b6d0c",
   "metadata": {},
   "source": [
    "# 2. Define CMNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5737b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cmnn_ensemble(input_dim, load_weights=False):\n",
    "    alpha11, alpha12, alpha13 = 1.0, 1.0, 1.0\n",
    "    alpha21, alpha22, alpha23 = 1.0, 1.0, 1.0\n",
    "    num_estimators = 2\n",
    "\n",
    "    ensemble, ensemble_weights = [], {}\n",
    "    for i in range(num_estimators):\n",
    "        f11 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 512,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 512, \n",
    "                    'activation': 'relu',\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        f12 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 512,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 512, \n",
    "                    'activation': 'relu', \n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        f13 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 512,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 512, \n",
    "                    'activation': 'relu', \n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        f21 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 512,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 512, \n",
    "                    'activation': 'relu', \n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        f22 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 512,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 512, \n",
    "                    'activation': 'relu', \n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        f23 = ICK(\n",
    "            kernel_assignment=['ImplicitDenseNetKernel'],\n",
    "            kernel_params={\n",
    "                'ImplicitDenseNetKernel':{\n",
    "                    'input_dim': input_dim,\n",
    "                    'latent_feature_dim': 512,\n",
    "                    'num_blocks': 0, \n",
    "                    'num_layers_per_block': 1, \n",
    "                    'num_units': 512, \n",
    "                    'activation': 'relu', \n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        if load_weights:\n",
    "            for f in ['f11', 'f12', 'f13', 'f21', 'f22', 'f23']:\n",
    "                eval(f).kernels[0].load_state_dict(torch.load('./checkpoints/cmick_jobs.pt')['model_'+str(i+1)][f])\n",
    "        else:\n",
    "            model_weights = {\n",
    "                'f11': f11.kernels[0].state_dict(), 'f12': f12.kernels[0].state_dict(), 'f13': f13.kernels[0].state_dict(), \n",
    "                'f21': f21.kernels[0].state_dict(), 'f22': f22.kernels[0].state_dict(), 'f23': f23.kernels[0].state_dict()\n",
    "            }\n",
    "            ensemble_weights['model_'+str(i+1)] = model_weights\n",
    "        # Set output_binary=True for binary Y0 and Y1\n",
    "        baselearner = CMICK(\n",
    "            control_components=[f11,f21], treatment_components=[f12,f22], shared_components=[f13,f23],\n",
    "            control_coeffs=[alpha11,alpha21], treatment_coeffs=[alpha12,alpha22], shared_coeffs=[alpha13,alpha23], \n",
    "            coeff_trainable=True, output_binary=True\n",
    "        )\n",
    "        ensemble.append(baselearner)\n",
    "    if not load_weights:\n",
    "        if not os.path.exists('./checkpoints'):\n",
    "            os.makedirs('./checkpoints')\n",
    "        torch.save(ensemble_weights, './checkpoints/cmick_jobs.pt')\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ae16d",
   "metadata": {},
   "source": [
    "# 3. Training and evaluation of CMNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac014cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_cmnn(ensemble, data_generators, data, lr, treatment_index=1): \n",
    "    # The index of \"T_train\" in \"data_train\" is 1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optim = 'sgd'\n",
    "    optim_params = {\n",
    "        'lr': lr, \n",
    "        'momentum': 0.99,\n",
    "        'weight_decay': 1e-4\n",
    "    }\n",
    "    epochs, patience = 1000, 10\n",
    "    trainer = CMICKEnsembleTrainer(\n",
    "        model=ensemble,\n",
    "        data_generators=data_generators,\n",
    "        optim=optim,\n",
    "        optim_params=optim_params, \n",
    "        model_save_dir=None,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        patience=patience, \n",
    "        treatment_index=treatment_index\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "    mean_test_pred, std_test_pred, y_test_true = trainer.predict()\n",
    "    y_test, t_test = data['Y_test'], data['T_test']\n",
    "    r_pol = policy_risk_binary(mean_test_pred, y_test, t_test)\n",
    "    print('Policy risk (CMNN):             %.4f' % (r_pol))\n",
    "    \n",
    "    return r_pol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169031d",
   "metadata": {},
   "source": [
    "# 4. Benchmark 1: original CMGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51e6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_original_cmgp(data):\n",
    "    X_train, T_train, Y_train = data['X_train'], data['T_train'], data['Y_train']\n",
    "    X_test, T_test, Y_test = data['X_test'], data['T_test'], data['Y_test']\n",
    "    cmgp_model = CMGP(X_train, T_train, Y_train)\n",
    "\n",
    "    mu0_test_pred, mu1_test_pred = cmgp_model.predict(X_test, return_var=False)\n",
    "    mu_test_pred = np.concatenate([mu0_test_pred, mu1_test_pred], axis=1)\n",
    "    r_pol = policy_risk_binary(mu_test_pred, Y_test, T_test)\n",
    "    print('Policy risk (CMGP):             %.4f' % (r_pol))\n",
    "    \n",
    "    return r_pol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9d0b9",
   "metadata": {},
   "source": [
    "# 5. Benchmark 2: CEVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7623a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_cevae(data):\n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "    batch_size = int(data['X_train'].shape[0]/8)\n",
    "    train_iters = 20000\n",
    "    eval_iters = 200\n",
    "    latent_dim = 20\n",
    "    n_h = 64\n",
    "    X_train, T_train, Y_train, X_test = torch.tensor(data['X_train']).float(), torch.tensor(data['T_train']).float(), \\\n",
    "                                        torch.tensor(data['Y_train']).float(), torch.tensor(data['X_test']).float()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    X_train = X_train.to(device)\n",
    "    T_train = T_train.to(device)\n",
    "    Y_train = Y_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    \n",
    "    # init networks (overwritten per replication)\n",
    "    p_x_z_dist = p_x_z(dim_in=latent_dim, nh=3, dim_h=n_h).to(device)\n",
    "    p_t_z_dist = p_t_z(dim_in=latent_dim, nh=1, dim_h=n_h, dim_out=1).to(device)\n",
    "    p_y_zt_dist = p_y_zt(dim_in=latent_dim, nh=3, dim_h=n_h, dim_out=1, output_binary=True).to(device)\n",
    "    q_t_x_dist = q_t_x(dim_in=X_train.shape[1], nh=1, dim_h=n_h, dim_out=1).to(device)\n",
    "\n",
    "    # t is not feed into network, therefore not increasing input size (y is fed).\n",
    "    q_y_xt_dist = q_y_xt(dim_in=X_train.shape[1], nh=3, dim_h=n_h, dim_out=1, output_binary=True).to(device)\n",
    "    q_z_tyx_dist = q_z_tyx(dim_in=X_train.shape[1]+1, nh=3, dim_h=n_h, dim_out=latent_dim).to(device)\n",
    "    p_z_dist = normal.Normal(torch.zeros(latent_dim).to(device), torch.ones(latent_dim).to(device))\n",
    "\n",
    "    # Create optimizer\n",
    "    params = list(p_x_z_dist.parameters()) + \\\n",
    "             list(p_t_z_dist.parameters()) + \\\n",
    "             list(p_y_zt_dist.parameters()) + \\\n",
    "             list(q_t_x_dist.parameters()) + \\\n",
    "             list(q_y_xt_dist.parameters()) + \\\n",
    "             list(q_z_tyx_dist.parameters())\n",
    "\n",
    "    # Adam is used, like original implementation, in paper Adamax is suggested\n",
    "    optimizer = optim.Adamax(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # init q_z inference\n",
    "    q_z_tyx_dist = init_qz(q_z_tyx_dist, Y_train, T_train, X_train).to(device)\n",
    "\n",
    "    # Training\n",
    "    loss = []\n",
    "    for _ in tqdm(range(train_iters)):\n",
    "        i = np.random.choice(X_train.shape[0],size=batch_size,replace=False)\n",
    "        Y_train_shuffled = Y_train[i,:].to(device)\n",
    "        X_train_shuffled = X_train[i,:].to(device)\n",
    "        T_train_shuffled = T_train[i,:].to(device)\n",
    "\n",
    "        # inferred distribution over z\n",
    "        xy = torch.cat((X_train_shuffled, Y_train_shuffled), 1).to(device)\n",
    "        z_infer = q_z_tyx_dist(xy=xy, t=T_train_shuffled)\n",
    "        # use a single sample to approximate expectation in lowerbound\n",
    "        z_infer_sample = z_infer.sample()        \n",
    "\n",
    "        # RECONSTRUCTION LOSS\n",
    "        # p(x|z)\n",
    "        x_con = p_x_z_dist(z_infer_sample)\n",
    "        # l1 = x_bin.log_prob(x_train).sum(1)\n",
    "\n",
    "        l2 = x_con.log_prob(X_train_shuffled).sum(1)\n",
    "\n",
    "        # p(t|z)\n",
    "        t = p_t_z_dist(z_infer_sample)\n",
    "        l3 = t.log_prob(T_train_shuffled).squeeze()\n",
    "\n",
    "        # p(y|t,z)\n",
    "        # for training use trt_train, in out-of-sample prediction this becomes t_infer\n",
    "        y = p_y_zt_dist(z_infer_sample, T_train_shuffled)\n",
    "        l4 = y.log_prob(Y_train_shuffled).squeeze()\n",
    "\n",
    "        # REGULARIZATION LOSS\n",
    "        # p(z) - q(z|x,t,y)\n",
    "        # approximate KL\n",
    "        l5 = (p_z_dist.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)).sum(1)\n",
    "\n",
    "        # AUXILIARY LOSS\n",
    "        # q(t|x)\n",
    "        t_infer = q_t_x_dist(X_train_shuffled)\n",
    "        l6 = t_infer.log_prob(T_train_shuffled).squeeze()\n",
    "\n",
    "        # q(y|x,t)\n",
    "        y_infer = q_y_xt_dist(X_train_shuffled, T_train_shuffled)\n",
    "        l7 = y_infer.log_prob(Y_train_shuffled).squeeze()\n",
    "\n",
    "        # Total objective\n",
    "        # inner sum to calculate loss per item, torch.mean over batch\n",
    "        loss_mean = torch.mean(l2 + l3 + l4 + l5 + l6 + l7)\n",
    "        loss.append(loss_mean.cpu().detach().numpy())\n",
    "        objective = -loss_mean\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate gradients\n",
    "        objective.backward()\n",
    "        # Update step\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Evaluation\n",
    "    Y0_pred, Y1_pred = [], []\n",
    "    t_infer = q_t_x_dist(X_test)\n",
    "\n",
    "    eval_iters = 1000\n",
    "    for _ in tqdm(range(eval_iters)):\n",
    "        ttmp = t_infer.sample()\n",
    "        y_infer = q_y_xt_dist(X_test, ttmp)\n",
    "\n",
    "        xy = torch.cat((X_test, y_infer.sample()), 1)\n",
    "        z_infer = q_z_tyx_dist(xy=xy, t=ttmp).sample()\n",
    "        y0 = p_y_zt_dist(z_infer, torch.zeros(z_infer.shape[0],1).to(device)).sample()\n",
    "        y1 = p_y_zt_dist(z_infer, torch.ones(z_infer.shape[0],1).to(device)).sample()\n",
    "        Y0_pred.append(y0.detach().cpu().numpy().ravel())\n",
    "        Y1_pred.append(y1.detach().cpu().numpy().ravel())\n",
    "\n",
    "    # sameple from the treated and control    \n",
    "    mu0_test_pred = np.mean(np.array(Y0_pred), axis=0)\n",
    "    mu1_test_pred = np.mean(np.array(Y1_pred), axis=0)\n",
    "    mu_test_pred = np.stack([mu0_test_pred, mu1_test_pred], axis=1)\n",
    "    Y_test, T_test = data['Y_test'], data['T_test']\n",
    "    \n",
    "    r_pol = policy_risk_binary(mu_test_pred, Y_test, T_test)\n",
    "    print('Policy risk (CEVAE):             %.4f' % (r_pol))\n",
    "    \n",
    "    return r_pol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740ed30",
   "metadata": {},
   "source": [
    "# 6. Benchmark 3: GANITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b35b63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_ganite(data):\n",
    "    X_train, T_train, Y_train = data['X_train'], data['T_train'], data['Y_train']\n",
    "    X_test, T_test, Y_test = data['X_test'], data['T_test'], data['Y_test']\n",
    "    model = Ganite(X_train, T_train, Y_train, dim_hidden=4, alpha=1, beta=5, minibatch_size=128, depth=3)\n",
    "    with torch.no_grad():\n",
    "        X_test = model._check_tensor(X_test).float()\n",
    "        y_test_pred = model.inference_nets(X_test).detach().numpy()\n",
    "    r_pol = policy_risk_binary(y_test_pred, Y_test, T_test)\n",
    "    print('Policy risk (GANITE):             %.4f' % (r_pol))\n",
    "    return r_pol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258ef01",
   "metadata": {},
   "source": [
    "# 7. Benchmark 4: CFRnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb76cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcafdd09",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_ratio, test_ratio = 0.56, 0.20\n",
    "    lr = 1e-4\n",
    "    data_generators, data = load_and_preprocess_data(train_ratio, test_ratio, random_state=1)\n",
    "    input_dim = data['X_train'].shape[1]\n",
    "    ensemble = build_cmnn_ensemble(input_dim, load_weights=False)\n",
    "    r_pol_cmnn = fit_and_evaluate_cmnn(ensemble, data_generators, data, lr)\n",
    "    r_pol_cmgp = fit_and_evaluate_original_cmgp(data)\n",
    "    r_pol_cevae = fit_and_evaluate_cevae(data)\n",
    "    r_pol_ganite = fit_and_evaluate_ganite(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
